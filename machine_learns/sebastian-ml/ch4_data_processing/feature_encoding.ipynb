{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Feature encoding \n",
    "Typically your model isn't going to understand categorical data. So we use feature encoding to help us transform categorical data into numeric data.\n",
    "\n",
    "---\n",
    "### Label encoding\n",
    "Transform categorical to numerical data by assigning a numerical value to each of the categories. So like `male=1` and `female=0`. The downside is that your model may treat the higher values as 'more important'. Imagine `New York=7`, but `London=112`, it would prioritize London as having more weight/influence, even though those are just unique identifiers that we picked.\n",
    "\n",
    "---\n",
    "### One hot encoding\n",
    "Use when independent variables are 'nominal', not in a specific order (e.g. Gender, Location). It'll create k different columns, where one is 1 (to indicate presence of a category) and the rest are 0 (to indicate absence). So if you had 3 locations 'New York, Indiana, and Texas', and the current record is located in 'Texas', then 'Texas=1', but Indiana and New York are zero. This is popular since all computers understand binary.\n",
    "\n",
    "---\n",
    "### Dummy encoding\n",
    "A technique where we avoid redundancy by dropping a column whose value we're able to infer based on the other columns. The reason we're removing a column is to avoid an issue in ML called multi-collinearity. \n",
    "- **Multi-collinearity:** Happens when two or more independent variables are correlated, like highly correlated. This leads to less accurate results, not in the sense of estimates, but in the sense of being able to figure out how much our independent variables (inputs) influence the dependent variable (output) individually ('weights' are inaccurate). I mean this is pretty important as some models assume each feature contributes to predicting the target variable. It's inevitable that there is some correlation between variables (0.3-0.7), but above that is eyebrow raising.\n",
    "\n",
    "---\n",
    "### Target Encoding\n",
    "Calculate the average of the dependent variable (y), and replace the categorical variable with that mean value. Though there are some pros and cons that should be known about target encoding. Though target encoding isn't commonly used.\n",
    "- **Advantages:** Doesn't add dimensionality to the dataset. Let me explain, when you do target encoding and create the 'Gender Encoded' column, you probably aren't going to use the original 'Gender' column anymore, so you're adding one and deleting one column. However in something like one-hot or dummy encoding you could be adding various extra columns.\n",
    "- **Disadvantages:** It's dependent on the target value's distribution, which means target encoding requires careful validation as it can be prone to over-fitting.\n",
    "\n",
    "---\n",
    "### Hash Encoder\n",
    "Encodes categorical variables into numerical values using a hash function. This is how it works:\n",
    "1. **Apply a hash function:** Each category is passed through a hash function (e.g. MD5, SHA-1) that converts the category into a fixed-length numeric vector.\n",
    "2. **Map to a smaller vector:** The result of the hash function is mapped to a smaller number of bins/buckets, which is typically smaller than the total number of unique categories. This is done through modulo operator, and it works exactly like how hashmaps work. Basically the color red hashes to `12345`, and then `12345 % 10 = 9`, so the hash value for red is assigned to bucket 9.\n",
    "\n",
    "---\n",
    "#### Example of hash encoding\n",
    "Let's say we have a `Color` feature with 1,000 unique values, but we don't want to create 1,000 new columns via one-hot encoding. With hash encoding, we use a hash function to convert each unique value into a smaller, fixed-size vector, say with 10 bins. Each color is assigned to one of those bins bashed on its hashed value.\n",
    "\n",
    "---\n",
    "#### Why use it and when to?\n",
    "Used when a feature has as a high cardinality (a lot of unique values). We can handle converting these features into numerical values, more efficiently, and without creating like a thousand new columns.\n",
    "This is especially useful when there are many categorical features, making your system very scalable, and very resource efficient if that's needed in your environment. \n",
    "\n",
    "The downsides are the ris kof collisions, which occurs when two distinct categories are put in the same bin. You can reduce this by using more bins, which uses more memory. Another downside is that things aren't very interpretable. I mean with one hot encoding, you have columns where you can understand them, but with hashed features, you can't really tell what the original category was.\n",
    "\n",
    "---\n",
    "### Def. Over-fitting:\n",
    "The scenario of when a model learns too much from the training data, including noise/irrelevant data. This causes the model to perform well on that specific training data, but poorly on new data. This is the idea of the model becoming too tailored or used to that specific set of data, capturing patterns that don't generalize well to other data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "df = pd.read_csv(\"./data/Churn_Modelling.csv\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obvious by now, but drop your un-needed columns\n",
    "df.drop(columns=['CustomerId', 'RowNumber', 'Surname'], inplace=True)\n",
    "\n",
    "# Let's pick gender for encoding, but first clean any missing values \n",
    "df[\"Gender\"] = df[\"Gender\"].fillna(\"Male\")\n",
    "\n",
    "# Okay after this we can start with encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "+ Label Encoding: Let's do label encoding by mapping 'Male' 1 and Femael\n",
    "\n",
    "NOTE: Always check the comments and documentation. For this sklearn function\n",
    "it's not recommended to do it on x variables, like how we're doing it now. Ideally\n",
    "you would do it on output/y/dependent variables such as \"exited\", but for the example \n",
    "we'll ignore this.\n",
    "'''\n",
    "\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "df['gender_label'] = labelEncoder.fit_transform(df['Gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "+ One Hot Encoding:\n",
    "\n",
    "'''\n",
    "\n",
    "one_hot = pd.get_dummies(df['Geography'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "+ Dummy Encoding: The 3 columns we're getting from one-hot are called dummy columns.\n",
    "We generate n dummy columns, but the idea is that we only need n-1 columns.\n",
    "\n",
    "'''\n",
    "df_dummies = pd.get_dummies(df,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "+ Target Encoding: Another encoding technique, but again notice how we're passing in\n",
    "an independent variable gender, and a dependent/target variable 'Exited'.\n",
    "\n",
    "'''\n",
    "from category_encoders import TargetEncoder\n",
    "encoder = TargetEncoder()\n",
    "\n",
    "df['Gender Encoded'] = encoder.fit_transform(df['Gender'], df['Exited'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
