{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "### Objectives\n",
    "- Removing and imputing missing values from a dataset \n",
    "- Getting cateogrical data into shape for machine learning algorithms\n",
    "- Selecting relevant features for model constructions\n",
    "\n",
    "## Dealing with missing data \n",
    "\n",
    "### Identifying missing values\n",
    "This can be caused by errors in data collection, certain measurements aren't applicable, or particular fields can be optional. To store data in-memory, you're definitely going to be using a Pandas DataFrame, which also uses optimized NumPy arrays.\n",
    "\n",
    "Here's an easy way to identify missing values.\n",
    "```Python \n",
    "df.isnull.sum()\n",
    "```\n",
    "\n",
    "---\n",
    "### Method 1: Eliminating samples or features\n",
    "Remove the corresponding features (columns) or samples (rows) from the dataset entirely. The risk to this is removing too many samples, or losing \n",
    "valuable feature columns. As a result, one of the other techniques is imputation.\n",
    "\n",
    "```Python \n",
    "# For any row that has one or more missing column values, remove the entire row\n",
    "df.dropna(axis=0)\n",
    "\n",
    "# Only drop rows where all of their column values are missing \n",
    "df.dropna(how='all')\n",
    "\n",
    "# Drop rows that have less than 4 actual values\n",
    "df.dropna(thresh=4)\n",
    "\n",
    "# Drop rows that that have specific columns missing; if column 'C' is missing for a row, drop the row.\n",
    "df.dropna(subset=['C'])\n",
    "\n",
    "# For any column, if one of its row values is missing, remove the entire column\n",
    "df.dropna(axis=1)\n",
    "```\n",
    "\n",
    "---\n",
    "### Method 2: Imputing missing values\n",
    "This is the idea of estimating what a missing value would be, based on the data we already have. The idea of interpolation. \n",
    "\n",
    "- **Mean imputation**: When we replace the value of the missing column with the mean value of the entire feature column. The arithmetic mean, but you can try other measures of center like the media nor mode. The latter being useful for imputing categorical features.\n",
    "\n",
    "\n",
    "Sci-kit has data **transformer** classes for imputing. They have methods like fit for learning the parameters from training data and transform to use those parameters to transform your data. However, there's also the estimator API. These have a `predict` and `transform` method:\n",
    "1. Training data and training labels are inputted into the `fit()` function, allowing the model to learn.\n",
    "2. The model then takes test data, and we use the `predict(X_test)`, which will output what the model thinks is the predicted class label. \n",
    "\n",
    "\n",
    "## Handling categorical data\n",
    "\n",
    "---\n",
    "### Premise: Nominal and ordinal features\n",
    "- **ordinal features:** Categorical values that can be sorted and ordered. E.g. t-shirt size is ordinal since we can define an order XL > L > M.\n",
    "- **nominal features:** Categorical values that don't imply any order. E.g. color of the t-shirt.\n",
    "As we've realized already, categorical values need to be converted into numerical representations for machine learning algorithms to work, as it all relies on math. Ordinal features work essentially the same too, but you'll need to have more considerations.\n",
    "\n",
    "---\n",
    "### Mapping ordinal features\n",
    "There's no convenient function that can derive the correct order the labels for the `size` feature. So we'll create our own. Let $XL=L+1=M+2$. Something like this would work:\n",
    "```Python\n",
    "size_mapping = {\n",
    "  'XL': 3,\n",
    "  'L': 2,\n",
    "  'M': 1\n",
    "}\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "```\n",
    "For example, now any rows with a size \"XL\" would now have the integer 3 in the size column. And so on according to our mapping.\n",
    "\n",
    "---\n",
    "### Encoding class labels\n",
    "Categorical class labels should be encoded/represented as integer values too. We'll just do something similar to mapping ordinal features, but since class labels aren't ordinal it doesn't matter which integer we assign to one. So we can simply enumerate from 0.\n",
    "```Python \n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique([df['class_label']]))}\n",
    "df['class_label'] = df['class_label'].map(class_mapping)\n",
    "```\n",
    "\n",
    "The more convenient way to do this is using `LabelEncoder` class from Sci-kit\n",
    "```Python\n",
    "class_label_encoder = LabelEncoder()\n",
    "y = class_label_encoder.fit_transform(df['class_label'].values)\n",
    "```\n",
    "\n",
    "---\n",
    "### One Hot Encoding for nominal features\n",
    "One of the big issues that we need to avoid when doing feature encoding is how your model may treat higher values as more important. Imagine of `red=1` but `yellow=10`, and it may think yellow has more weight/influence, even though those are just unique identifiers that we just picked.\n",
    "\n",
    "The solution to this would be the idea of one hot encoding. Of course this is going to create a lot more columns, so there's that to think about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning data into training and testing sets\n",
    "\n",
    "\n",
    "### Premise \n",
    "We're going to be looking at the Wine dataset, preprocess the data, do feature selection techniques to reduce the dimensionality of the dataset.\n",
    "\n",
    "### Wine dataset\n",
    "- 13 different features\n",
    "- 178 samples\n",
    "- Each sample has one of three class labels {1,2,3}, which refer to the 3 different types of grapes grown in the same region in Italy. \n",
    "\n",
    "\n",
    "p188s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "'machine-learning-databases/wine/wine.data',\n",
    "header=None)\n",
    "\n",
    "df.columns = ['Class label', 'Alcohol',\n",
    " 'Malic acid', 'Ash',\n",
    " 'Alcalinity of ash', 'Magnesium',\n",
    " 'Total phenols', 'Flavanoids',\n",
    " 'Nonflavanoid phenols',\n",
    " 'Proanthocyanins',\n",
    " 'Color intensity', 'Hue',\n",
    " 'OD280/OD315 of diluted wines',\n",
    " 'Proline']\n",
    "\n",
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
